# ğŸ“˜ Lesson 0 Summary: Foundations of LLMs and Prompting

---

## ğŸ”¤ **What Are Tokens?**

- ğŸ§© **Tokens** are the basic units of text used by LLMs â€” not always full words.
- ğŸª„ A word like `"unbelievable"` might be split into: `["un", "believ", "able"]`
- ğŸ“ Token count affects:
  - ğŸ“¦ **Prompt length limits**
  - ğŸ’¸ **Cost (API-based models often bill by token)**
  - ğŸ§  **Memory strategies in agents**

---

## ğŸ”¥ **Temperature â€“ Controlling Randomness**

| Temperature | Behavior                               | Use Case                         |
|-------------|----------------------------------------|----------------------------------|
| `0.0`       | ğŸ”’ Fully deterministic                 | Tool use, code, data parsing     |
| `0.7`       | ğŸ¯ Balanced, slightly creative         | Conversations, planning          |
| `1.0+`      | ğŸ¨ High creativity, less predictability | Brainstorming, story writing     |

- **High temp** = more diverse outputs  
- **Low temp** = more reliable and repeatable

---

## ğŸ¯ **Top-p â€“ Controlling the Sampling Pool**

> Top-p (nucleus sampling) controls how many tokens are considered by **cumulative probability**, not by count.

- `top-p = 1.0` â†’ Use **all** token options (default behavior)
- `top-p = 0.9` â†’ Limit to tokens that **together** account for 90% of total likelihood
- `top-p = 0.3` â†’ Only sample from the **most likely** tokens

ğŸ” **Think of it as a safety net** â€” it cuts off rare/outlier token options.

---

## ğŸ§  **LLMs Are Stateless â€“ Agents Fix That**

LLMs forget everything between calls unless you include the full context in the prompt.

ğŸ¤– **Agents fix this by adding:**
- ğŸ—ƒï¸ **Memory**: past inputs, reflections, scratchpads
- ğŸ› ï¸ **Tools**: APIs, search, plugins
- ğŸ§­ **Planning**: multi-step goals and strategies

---

## ğŸ§ª **How Developers Actually Tune Temperature & Top-p**

âœ… They **do not** manually tweak settings per prompt. Instead, they:

- Run **parameter sweeps** and grid search
- Use **automated evals** and scoring
- Monitor with tools like:
  - [LangSmith](https://smith.langchain.com)
  - [AgentOps](https://www.agentops.ai/)
  - [TruLens](https://www.trulens.org/)
  - Custom logging dashboards
- Dynamically adjust settings in agents depending on the task type

ğŸ§  Example in an agent:
```python
if task == "call_api":
    temperature = 0.0
elif task == "write_tweet":
    temperature = 0.9
